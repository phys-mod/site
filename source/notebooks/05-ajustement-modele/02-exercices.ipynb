{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chute libre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des étudiants ont effectués une expérience de chute libre: ils ont fait tomber un objet de chaque étage de l'Atrium, sans vitesse initiale, et ont mesuré à chaque fois le temps de chute $T$ de l'objet ainsi que la hauteur de l'étage $H$. On reporte dans le tableau ci-dessous le résultat des mesures pour 4 groupes différents.\n",
    "\n",
    "![data chute libre](./table_analyse_chute.png)\n",
    "\n",
    "Nous savons que l'objet chute sous l'effet de la pesanteur. Nous voulons grâce à cette expérience déterminer la magnitude de la pesanteur. Pour cela, nous utilisons un modèle simple du temps de chute libre. On montre facilement d'après la loi fondamentale de la dynamique, si on néglige les frottements de l'air, le lien entre la hauteur de l'étage $H$, le temps de chute $T$, et la magnitude de la pesanteur $g$:\n",
    "\n",
    "$$\n",
    "H = g\\times\\dfrac{T^2}{2}\n",
    "$$\n",
    "\n",
    "1. Identifier dans cette équation  le(s) donnée(s) de l'expérience et le(s) paramètre(s) du modèle. Écrire les données sous la forme de tableaux numpy.\n",
    "2. Écrire le modèle sous la forme d'une fonction python à ajuster, de la forme $y(x;a)=ax$.\n",
    "3. Déterminer la valeur de $g$ en ajustant les données du 1er groupe. Afficher à l'écran la valeur trouvée, et faire un graphique sur lequel on voit le modèle (en ligne continue) et les données (marqueurs). On veillera à légender et mettre des titres aux axes et au graphique.\n",
    "4. Ajuster les données des 4 groupes grâce à une boucle, et calculer la valeur de $g$ comme la moyenne des résultats des 4 groupes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "# Ecriture des données dans un tableau numpy\n",
    "T = np.array([[1.09,  1.39,  1.70,  2.05,  2.25],\n",
    "              [1.17,  1.54,  1.72,  2.18,  2.40],\n",
    "              [1.33,  1.68,  1.56,  2.19,  2.40],\n",
    "              [0.95,  1.61,  1.49,  2.16,  2.18]])\n",
    "H = np.array( [7.40,  10.80, 14.20, 17.60, 20.95])\n",
    "# Modèle\n",
    "def chute(x,a):\n",
    "    return a*x\n",
    "# fit des données du 1er groupe\n",
    "y = H\n",
    "x = T[0,:]**2/2\n",
    "params, covar = curve_fit(chute,x,y)\n",
    "print(\"Groupe 1:\")\n",
    "print(\"  g = \", params[0], \" m/s^2\")\n",
    "# Graphique\n",
    "xm = np.linspace(0,3,100)\n",
    "plt.plot(xm,chute(xm,params[0]))\n",
    "plt.plot(x,y,'*')\n",
    "plt.axis([0,3,0,25])\n",
    "plt.legend(('Modèle','Données'))\n",
    "plt.title('Fit des données du groupe 1')\n",
    "plt.xlabel('T**2/2 [s**2]')\n",
    "plt.ylabel('H [m]')\n",
    "plt.show()\n",
    "# fit des données de tous les groupes\n",
    "Ng = 4\n",
    "g = np.empty(4)\n",
    "for i in range(4):\n",
    "    # fit des données du ieme groupe\n",
    "    y = H\n",
    "    x = T[i,:]**2/2\n",
    "    params, covar = curve_fit(chute,x,y)\n",
    "    print(\"Groupe \", str(i+1), \":\")\n",
    "    print(\"  g = \", params[0], \" m/s^2\")\n",
    "    g[i] = params[0]\n",
    "    # Graphique\n",
    "    plt.plot(xm,chute(xm,params[0]))\n",
    "    plt.plot(x,y,'*')\n",
    "plt.show()\n",
    "print(\"Valeur Moyenne:\")\n",
    "print(\"  g = \", np.mean(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loi de puissance\n",
    "On va travailler sur les données des exoplanetes, déjà étudiées en 2eme semaine. Le but de l'exercice consiste à déterminer si les données expérimentales peuvent etre ajustées par la fonctioncorrespondant à la 3eme loi de Kepler.\n",
    "\n",
    "1. Charger les données des exoplanetes du fichier `exoplanets.dat` et representer $\\log(R)$ en fonction de $\\log(T)$\n",
    "2. Expliquer pourquoi la représentation graphique logarithmique est mieux adaptée que la représentation graphique linéaire.\n",
    "3. Montrer également (par le calcul) qu'il est possible de faire une régression linéaire en utilisant la représentation mathématique logarithmique pour ajuster la 3eme loi de Kepler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#1\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import astropy.constants as cons\n",
    "au = cons.au.value\n",
    "s = 24*3600\n",
    "\n",
    "d_exp = np.loadtxt('../etu-02-python-intermediaire/exoplanets.dat')\n",
    "R = d_exp[:,0]*au\n",
    "T = d_exp[:,1]*s\n",
    "\n",
    "plt.figure()\n",
    "lR = np.log(R)\n",
    "lT = np.log(T)\n",
    "plt.plot(lR,lT,'.')\n",
    "plt.xlabel(\"$\\log(R)$\")\n",
    "plt.ylabel(\"$\\log(T)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "**Question 2**\n",
    "L'échelle logarithmique permet de voir toutes les données dispersées sur plusieurs ordres de grandeur.\n",
    "\n",
    "**Question 3**\n",
    "De plus, si $T^2 = k R^3$, alors $\\log(T) = \\frac{3}{2} \\log(R) + \\frac{1}{2} \\log(k)$ \n",
    "\n",
    "==> On peut également tester la relation de proportionalité via un ajustement affine des distributions logarithmiques. On attend alors une pente de 1.5 et une ordonnée à l'origine égale à 0.5log(k).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# 3\n",
    "def reglin(xi, yi):\n",
    "    '''\n",
    "        Calcul les paramètres a,b du meilleur ajustement d'un modèle linéaire\n",
    "        y = ax + b sur un jeu de données (xi, yi)\n",
    "    '''\n",
    "    N = np.size(xi)\n",
    "    \n",
    "    xm = np.mean(xi)\n",
    "    ym = np.mean(yi)\n",
    "    cov = 1/N * np.sum(xi*yi) - xm*ym\n",
    "    var = 1/N * np.sum(xi**2) - xm**2\n",
    "    \n",
    "    a = cov / var\n",
    "    b = ym - a*xm\n",
    "    \n",
    "    return (a,b)\n",
    "\n",
    "# Modèle\n",
    "def line(x,a,b):\n",
    "    return a*x+b\n",
    "\n",
    "a,b = reglin(lR,lT)\n",
    "print('a= ', a,'  b= ',b)\n",
    "k = np.exp(2*b)\n",
    "print(\"k=\",k,\"kg/N/m2\")\n",
    "\n",
    "plt.figure()\n",
    "x = np.linspace(min(lR),max(lR))\n",
    "plt.plot(lR,lT,'.',x,a*x+b)\n",
    "plt.xlabel(\"$\\log(R)$\")\n",
    "plt.ylabel(\"$\\log(T)$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Décroissance exponentielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ici tenter de modéliser la vitesse de décroissance de l'épaisseur de mousse de bière. On reprendra pour cela les données présentées par Arnd Leike, chercheur à l'Université Ludwig Maximilians de Munich, dans une publication  qui lui a valu le prix Ig Nobel en 2002.\n",
    "\n",
    "Récupérer les données présentées dans le tableau 1 de la publication disponible au lien suivant: disponible ici: https://www.tf.uni-kiel.de/matwis/amat/iss/kap_2/articles/beer_article.pdf. \n",
    "\n",
    "1. Représentez le logarithme décimal de l'epaisseur de mousse de la Augustinerbräu en fonction du temps: $\\log_{10}(h) = f(t)$. \n",
    "\n",
    "2. Effectuez l'ajustement qui vous semble le plus approprié, et en déduire le modèle de décroissance auquel cet ajustement correspond. Déterminez le  temps caractéristique de la décroissance \n",
    "\n",
    "3. Effectuer un ajustement exponentiel de la fonction $h(t)$ (cf résultat ci-dessous), et comparez vos résultats à ceux de la question précédente.\n",
    "\n",
    "4. Le taux de désintégration d'un matériau radioactif suit également une loi exponentielle. Peut-on trouver une analogie avec l'épaisseur de la mousse de bière?\n",
    "\n",
    "Dans le tableau 1 de l'article de Leike, l'auteur a également associé des incertitudes aux mesures d'épaisseur de mousses. \n",
    "\n",
    "5. Représentez ces incertitudes graphiquement en utilisant la fonction `errorbar` de matplotlib (cf résultat ci-dessous).\n",
    "\n",
    "\n",
    "Il est possible, dans l'ajustement d'un modèle $y=f(x)$ aux données, de pondérer chaque mesure $y_i$ par son incertitude $\\sigma_{yi}$.  Pour se faire, on minimise en fait sur sa mesure dans l'ajustement la quantité appelée \"Khi-2\", et définie par:\n",
    "\n",
    "$$ \n",
    "\\chi^2 = \\Sigma  \\frac{(y_i - f(x_i))^2 }{{\\sigma_{yi}}^2} \n",
    "$$\n",
    "\n",
    "6. Reprendre la question 3 en passant les erreurs $\\sigma_yi$ dans le paramètre *sigma* de la fonction `curve_fit`. On veillera à remplacer les valeurs d'incertitudes nulles par une valeur > 0 pour permettre la convergence de l'ajustement (cf equation précédente).\n",
    "\n",
    "On pourra remarquer qu'en plus des paramètres ajustés, un 2eme paramètre est retournée par la fonction `curve_fit`. Il s'agit de leur matrice de covariance de ces paramètres (que l'on peut noter `pcov`, dont on peut extraire l'incertitude sur chacun des paramètres, donnée par  `perr = np.sqrt(np.diag(pcov))`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#1\n",
    "# Données\n",
    "h = np.array([14.0, 11.8, 10.5, 9.3, 8.5, 7.7, 7.1, 6.5, 6.0, 5.3, 4.4, 3.5, 2.9, 1.3, 0.7])  #cm\n",
    "t = np.array([0, 15, 30, 45, 60, 75, 90, 105, 120, 150, 180, 210, 240, 300, 360]) #s\n",
    "lh = np.log(h)\n",
    "\n",
    "plt.plot(t,lh,'+')\n",
    "plt.xlabel('Temps (s)')\n",
    "plt.ylabel('$\\log(h)$')\n",
    "\n",
    "def modlin(x, a, b):\n",
    "    return a + b*x\n",
    "\n",
    "# Ajustement linéaire\n",
    "params, covar = curve_fit(modlin,t,lh,[14,-1])\n",
    "print(\"(a,b)=\",params)\n",
    "print(\"tau=\",-1/params[1],\"s\")\n",
    "t_model = np.linspace(min(t),max(t))\n",
    "y_model = params[0]+params[1]*t_model\n",
    "plt.plot(t_model,y_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#3.\n",
    "def expLaw(x,A0,tau):\n",
    "    return A0*np.exp(-x/tau)\n",
    "\n",
    "plt.plot(t,h,'+')\n",
    "plt.xlabel('Temps (s)')\n",
    "plt.ylabel('h (cm)')\n",
    "\n",
    "params, covar = curve_fit(expLaw,t,h,[14,200])\n",
    "print(params)\n",
    "t_model = np.linspace(min(t),max(t))\n",
    "y_model = params[0]*np.exp(-t_model/params[1])\n",
    "plt.plot(t_model,y_model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "On remarquera que l'ajustements linéaire de $log(h)$ et l'ajustement exponentiel de $h$ produisent des valeurs différentes de la constante de temps $\\tau$ caractéristique de la décroissance déinie par $y = e^{\\frac{-t}{\\tau}}$: 131 s contre 148 s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Question 5\n",
    "\n",
    "dh = np.array([0.01, 0.3, 0.3, 0.5, 0.6, 0.6, 0.7, 0.8, 0.8, 1.1, 1.2, 0.9, 1.1, 0.7, 0.5])\n",
    "\n",
    "plt.errorbar(t,h,dh)\n",
    "params, pcov = curve_fit(expLaw,t,h,[14,200],sigma=dh)\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "print(params)\n",
    "print(perr)\n",
    "print('Tau = ',params[1],'+-',perr[1],'s')\n",
    "t_model = np.linspace(min(t),max(t))\n",
    "y_model = params[0]*np.exp(-t_model/params[1])\n",
    "plt.plot(t_model,y_model)\n",
    "plt.xlabel('Temps (s)')\n",
    "plt.ylabel('h (cm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustement sinusoidal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant travailler sur mes mesures de température effectué à la station météorolgique de Montélimar, qui serviront également pour le mini-projet. Ces données sont disponibles sur le site https://www.ecad.eu/. \n",
    "\n",
    "1. Chargez et représentez les températures quotidiennes sur la décennie 1970.  \n",
    "2. On observe très clairement des fluctuations saisonnières dans ces données. Proposez un ajustement pour les modéliser et réalisez le.\n",
    "\n",
    "**Remarques**\n",
    "\n",
    "Il est conseillé d'utiliser des variables de type `numpy.datetime64` pour manipuler les dates. \n",
    "\n",
    "Suivant la version de matplotlib que vous utilisez, il est cependant possible que vous ayez des problèmes pour représenter ce type de variables. On pourra alors choisir de les transformer en variables réelles, en utilisant la commande `astype(float)`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Récupération des données\n",
    "a = np.loadtxt('ECA_blended_custom/TG_STAID000786.txt',skiprows = 20,delimiter=',')\n",
    "station_id = a[:,0]\n",
    "date = a[:,1]\n",
    "T = a[:,2]/10\n",
    "Q = a[:,3] # facteur de qualité\n",
    "# Use valid data only\n",
    "date = date[Q == 0]\n",
    "T = T[Q == 0]\n",
    "\n",
    "# Construction de la variable de temps\n",
    "dt = []\n",
    "for d in date:\n",
    "    Y = str(d)[0:4]\n",
    "    M = str(d)[4:6]\n",
    "    d = str(d)[6:8]\n",
    "    #print(Y,M,d)\n",
    "    dt.append(np.datetime64(Y+'-'+M+'-'+d))\n",
    "dt = np.array(dt)  # On enregistre les variables d etemps comme un numpy.array de datetime64.\n",
    "\n",
    "print(\"Il y a\",len(T),\" mesures de température dans ce fichier entre\",min(dt),\",et\",max(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Représentation\n",
    "sel = np.logical_and(dt>np.datetime64('1970-01'),dt<np.datetime64('1980-01'))\n",
    "\n",
    "dt1970 = dt[sel]\n",
    "dtf1970 = dt1970.astype(float)\n",
    "T1970 = T[sel]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(dt1970,T1970)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('T')\n",
    "plt.xlim(min(dt1970),max(dt1970))\n",
    "\n",
    "# Ajustement sinusoidal\n",
    "def sinLaw(x,a,T0,phi,c):\n",
    "    w = 2*np.pi/T0\n",
    "    return a*np.sin(w*x+phi)+c\n",
    "\n",
    "params, covar = curve_fit(sinLaw,dt1970,T1970,[10,365,0,np.mean(T)])\n",
    "perr = np.sqrt(np.diag(covar))\n",
    "\n",
    "# Attention: pour calculer la fonction, on doit utiliser un tableau de réels plutot que de datetime64\n",
    "y_model = params[0]*np.sin(2*np.pi/params[1]*dtf1970+params[2])+params[3]\n",
    "plt.plot(dt1970,y_model)\n",
    "print(params)\n",
    "print(perr)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
